<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Running Diffusers on Remote, Self-Hosted Hardware

This page demonstrates how to use diffusers on remote, self-hosted hardware, with easy and consistent dependency setup
across environments. You can develop fully locally on a Colab or local Python script, while running select code on a
remote cluster to take advantage of remote hardware.

If you have cloud credentials for AWS, GCP, Azure, or Lambda, there is on-demand cluster support to automatically set up
and spin up/down clusters on these clouds, and send code to be run remotely.

If you have SSH credentials that you use to log in to an existing cluster, you can run code remotely there as well.

This tutorial is also available as a
[Colab](https://colab.research.google.com/drive/1Pc28iA1rtXC5y4xH-kblrfAmcNihHCYA?usp=sharing)

## Installation and Setup

This tutorial uses [Runhouse](https://github.com/run-house/runhouse) to demonstrate how you can locally send code and
functions to be run on remote hardware. Runhouse uses [SkyPilot](https://github.com/skypilot-org/skypilot) to handle
on-demand clusters on the cloud, or you can set up an existing cluster using SSH creds.

To install Runhouse:

```cli
pip install runhouse
```

### On-Demand Clusters (AWS, Azure, GCP, LambdaLabs)

To set up cloud credentials locally for any provider you have access to, run `sky check` and follow the instructions
provided to enable clouds for SkyPilot. Once this is configured properly, you will be able to launch on-demand clusters
locally, specifying specific instance types, or even launching on the cheapest provider!

Once cloud credentials are set up, launching a cluster is as the following line, which creates a cluster with the name
`rh-a10-diffusers`, with one `A10` accelerator, using the cheapest provider at the moment for the specified resources.

```python
gpu = rh.cluster(name="rh-a10-diffusers", instance_type="A10:1", provider="cheapest").up_if_not()
```

You can also set an autostop time, after which amount of inactivity on the server, the cluster will automatically
terminate. (Setting `-1` will turn off autostop.)

```python
gpu.autostop_mins = 60
```

To check the clusters you have launched, and their resources and status, run `sky status`.

To terminate the instance, you can run `sky down rh-a10-diffusers` from CLI, or `gpu.teardown()` in Python.

### On-Premise Cluster

If you have an on-prem cluster that you generally SSH into, you can also create a local cluster object to refer to that
cluster, as follows:

```python
gpu = rh.cluster(
    ips=["<ip of the cluster>"], ssh_creds={"ssh_user": "...", "ssh_private_key": "<path_to_key>"}, name="rh-a10x"
)
```

## Define Local Diffusers Functions

Once we're set up with our cluster, we can define our diffusers functions, just as we would a normal function. Here, we
we define a simple function that takes loads a pretrained Stable Diffusion Pipeline from a given model ID, generates
images of the given prompt, and returns those images.

```python
def load_run_pipeline(prompt, num_images=4, model_id="stabilityai/stable-diffusion-2-base"):
    from diffusers import StableDiffusionPipeline
    import torch

    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, revision="fp16").to("cuda")
    return pipe([prompt] * num_images).images
```

## Send and Run Local Function on Cluster

To run the function on a remote cluster, we create a Runhouse function `rh.function`, passing in the corresponding
function callable `load_run_pipeline`. We additionally specify the requirements `reqs` and system (cluster) `gpu`,
and send our function `to` the remote cluster. Just like PyTorch can send Tensors to different devices (CPU, CUDA),
you too can send functions to different hardware.

```python
reqs = ["./", "diffusers", "transformers", "torch --upgrade --extra-index-url https://download.pytorch.org/whl/cu117"]

load_run_pipeline_gpu = rh.function(fn=load_run_pipeline).to(system=gpu, reqs=reqs)
```

To run the function, call the rh function, load_run_pipeline_gpu, passing in arguments just as you would with the
original function. The behavior is exactly the same as the original function, except that the computation happens
on the specified system/cluster, rather than on local hardware. The result of the function call is returned locally,
where you can continue developing.

Note that this may take a few minutes to run, as the pretrained model is being downloaded.

```
prompt = "A hot dog made of matcha powder."
images = load_run_pipeline_gpu(prompt)
```

## Advanced Usages
Now that we have the basics for launching Stable Diffusion on remote, self-hosted hardware, we can explore some more
advanced usages.

### Pinning Stable Diffusion models for later use

You can pin and retrieve objects on the remote hardware, to be saved for later usage. In our case, if we want to run
repeated inference using the same Stable Diffusion model, we may want to pin it to hardware memory, so that future runs
can skip the model downloading part and run faster.

We accomplish this using `rh.pin_to_memory(<name>, <object>)`, and `object = rh.get_pinned_object(<name>)`. To clear
pins to save memory once you are done using the object, you can call `rh.clear_pins`

```python
def sd_pipe_pinned(prompt, num_images=4, model_id="stabilityai/stable-diffusion-2-base", revision="fp16"):
    import torch
    import runhouse as rh
    from diffusers import StableDiffusionPipeline, DDIMScheduler

    pipe = rh.get_pinned_object(model_id)

    # pin to memory if it is not in memory yet
    if pipe is None:
        pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, revision=revision).to(
            "cuda"
        )
        pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)
        rh.pin_to_memory(model_id, pipe)

    return pipe(prompt, num_images_per_prompt=num_images).images
```

```python
sd_pipe_pinned_gpu = rh.function(sd_pipe_pinned).to(system=gpu)
images = sd_pipe_pinned_gpu(prompt)
```

The first run above downloads the model and pins it to memory, running the function in ~30 seconds. Below, we reuse the
pinned model to have it run in just ~10 seconds.

```python
images = sd_pipe_pinned_gpu(prompt)
```

### Saving and rerunning Function
If you have an account with Runhouse, you can save and load these functions from different environments or onto
different clusters.

```python
rh.login(download_secrets=True, download_config=True, interactive=True)
# or
# runhouse login  # on CLI
```

To save the function and assign it a name:

```python
sd_pipe_pinned_gpu.save("sd_pipe_pinned")
```

And to reuse the function anywhere you've logged in with Runhouse:

```python
saved_pipeline = rh.function(name="sd_pipe_pinned")
images = saved_pipeline(prompt, num_images=4)
```
