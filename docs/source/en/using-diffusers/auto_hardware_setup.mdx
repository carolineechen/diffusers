<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Running Diffusers on Remote, Self-Hosted Hardware

This page demonstrates how to use diffusers on remote, self-hosted hardware, with easy and consistent dependency setup
across environments. You can develop fully locally on a Colab or local Python script, while running select code on a
remote cluster to take advantage of remote hardware.

Remote hardware includes both on-demand clusters through cloud providers and existing clusters with SSH credentials.

This tutorial is also available as a
[Colab](https://colab.research.google.com/drive/1Pc28iA1rtXC5y4xH-kblrfAmcNihHCYA?usp=sharing)

## Instantiate Remote Cluster

Install Runhouse, which is used for creating a local Cluster object and sending remote function calls.

```python
import runhouse as rh
```

### On-Demand Clusters (AWS, Azure, GCP, LambdaLabs)

On-Demand clusters are cloud clusters that are spun up/down automatically for you.
For instructions on setting up cloud access for on-demand clusters, please refer to
[Hardware Setup](https://runhouse-docs.readthedocs-hosted.com/en/main/rh_primitives/cluster.html#hardware-setup).

```python
gpu = rh.cluster(name="rh-a10-diffusers", instance_type="A10:1", provider="cheapest").up_if_not()
gpu.autostop_mins = 60  # automatically terminate gpu after this long of inactivity (-1 to disable)
```

### On-Premise Cluster

To instantiate a BYO or on-prem cluster, you will need to provide the IP addresses and SSH credentials as follows.

```python
gpu = rh.cluster(
    ips=["<ip of the cluster>"], ssh_creds={"ssh_user": "...", "ssh_private_key": "<path_to_key>"}, name="rh-a10x"
)
```

## Define Local Diffusers Functions

Once we're set up with our cluster, we can define our diffusers functions, just as we would a normal function. Here, we
we define a simple function that takes loads a pretrained Stable Diffusion Pipeline from a given model ID, generates
images of the given prompt, and returns those images.

```python
def load_run_pipeline(prompt, num_images=4, model_id="stabilityai/stable-diffusion-2-base"):
    from diffusers import StableDiffusionPipeline
    import torch

    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, revision="fp16").to("cuda")
    return pipe([prompt] * num_images).images
```

## Send and Run Local Function on Cluster

To run the function on a remote cluster, we create a Runhouse function `rh.function`, passing in the corresponding
function callable `load_run_pipeline`. We additionally specify the requirements `reqs` and system (cluster) `gpu`,
and send our function `to` the remote cluster. Just like PyTorch can send Tensors to different devices (CPU, CUDA),
you too can send functions to different hardware.

```python
reqs = ["./", "diffusers", "transformers", "torch --upgrade --extra-index-url https://download.pytorch.org/whl/cu117"]

load_run_pipeline_gpu = rh.function(fn=load_run_pipeline).to(system=gpu, reqs=reqs)
```

To run the function, call the rh function, load_run_pipeline_gpu, passing in arguments just as you would with the
original function. The behavior is exactly the same as the original function, except that the computation happens
on the specified system/cluster, rather than on local hardware. The result of the function call is returned locally,
where you can continue developing.

Note that this may take a few minutes to run, as the pretrained model is being downloaded.

```
prompt = "A hot dog made of matcha powder."
images = load_run_pipeline_gpu(prompt)
```
